{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Week 09 - Machine Learning with Scikit-learn\n",
        "\n",
        "Vaishnavi Tapetla"
      ],
      "metadata": {
        "id": "Gvx6J7SLR7HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this week’s assignment, you are required to investigate the accuracy-computation time tradeoffs of the different optimization algorithms (solvers) that are available for fitting linear regression models in Scikit-Learn. Using the code shared via the Python notebook (part of this week’s uploads archive) where the use of logistic regression was demonstrated, complete the following operations:\n",
        "\n",
        "Question 1\n",
        "\n",
        "\n",
        "\n",
        "Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The get_results(result_scores) function displays a summary of final results which shows the accuracy statistics for training and testing of each model. The model selection should focus on identifying the best-performing algorithm which demonstrates strong performance on training data and test data especially emphasizing test data results to verify generalization capabilities.\n",
        "According to the evidence the Random Forest model without cross-validation presented the highest training accuracy of 0.9993 but simultaneously displayed severe overfitting by performing poorly on the test data with an accuracy of 0.686. The standard Logistic Regression model and the Logistic Regression with L1 penalty and C=10 parameter achieved 0.718 test set accuracy as their best performance. These models exhibited similar training accuracy levels at 0.7333 as well as 0.7347 while demonstrating no evidence of overfitting. The Random Forest models with cross-validation likely achieved optimal training-test performance yet the visible output did not show their final scores.\n",
        "The Logistic Regression model with L1 penalty and C=10 demonstrates the best overall performance because it obtained 0.718 test accuracy along with 0.7347 training accuracy. The model strikes an optimal balance between training data fitting and unseen data prediction capability which makes it the most suitable choice for this classification problem."
      ],
      "metadata": {
        "id": "vm02JS8xR2vI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CRe16B5FRsp0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from patsy import dmatrices\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data = pd.read_csv('PatientAnalyticFile.csv')\n",
        "patient_data['mortality'] = np.where(patient_data['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "# Convert DateOfBirth to datetime and calculate age\n",
        "patient_data['DateOfBirth'] = pd.to_datetime(patient_data['DateOfBirth'])\n",
        "patient_data['Age_years'] = ((pd.to_datetime('2015-01-01') - patient_data['DateOfBirth']).dt.days/365.25)\n",
        "\n",
        "# Creating formula\n",
        "vars_remove = ['PatientID', 'First_Appointment_Date', 'DateOfBirth',\n",
        "               'Last_Appointment_Date', 'DateOfDeath', 'mortality']\n",
        "vars_left = set(patient_data.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)"
      ],
      "metadata": {
        "id": "qDftsn7vSe6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, patient_data)\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, np.ravel(Y), test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PtXySBfLSjds"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {\n",
        "    'Solver': [],\n",
        "    'Training Accuracy': [],\n",
        "    'Holdout Accuracy': [],\n",
        "    'Time Taken (seconds)': []\n",
        "}"
      ],
      "metadata": {
        "id": "nHslauymSlHb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create and fit model\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # Calculate accuracies\n",
        "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "    # Store results\n",
        "    results['Solver'].append(solver)\n",
        "    results['Training Accuracy'].append(round(train_accuracy, 4))\n",
        "    results['Holdout Accuracy'].append(round(test_accuracy, 4))\n",
        "    results['Time Taken (seconds)'].append(round(time_taken, 4))\n",
        "\n",
        "# Convert results to DataFrame for better display\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxt9gl5XSnXg",
        "outputId": "d1bb64b5-f6a6-4c69-9e09-77224baa4323"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4OXFp_wSz0M",
        "outputId": "7e436d9c-9e2d-4803-a5d2-ac7668ff1505"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Solver  Training Accuracy  Holdout Accuracy  Time Taken (seconds)\n",
            "0  newton-cg             0.7481            0.7362                0.2802\n",
            "1      lbfgs             0.7483            0.7362                2.2068\n",
            "2  liblinear             0.7479            0.7362                0.1638\n",
            "3        sag             0.7481            0.7362               24.2829\n",
            "4       saga             0.7480            0.7362               10.9742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx6YLLzUSp1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The five solvers demonstrated equivalent performance based on accuracy metrics. The training accuracy scores fall between 0.7479 and 0.7483 where lbfgs reaches 0.7483 as its peak value. The 0.0004 point difference between the top and bottom scoring solver is so small that it probably does not create a statistically meaningful distinction. All solvers demonstrated the same holdout accuracy score of 0.7362 which indicates their equal capability to predict new data points.\n",
        "The computational efficiency becomes the decisive factor since all accuracy metrics are equivalent. The execution times between solvers differ significantly from one another. The liblinear solver demonstrated the fastest execution time at 0.1638 seconds before newton-cg completed at 0.2802 seconds. The time required for sag and saga to execute reached 24.2829 seconds and 10.9742 seconds respectively whereas lbfgs finished at 2.2068 seconds.\n",
        "Among the three metrics the liblinear solver demonstrates the highest performance level. The liblinear solver reached identical accuracy levels as other solvers yet outperformed them in speed by 1.7 times newton-cg, 13.5 times lbfgs, 67 times saga, and 148 times sag. The computational efficiency of liblinear solver provides substantial speed benefits to model retraining processes without affecting performance results."
      ],
      "metadata": {
        "id": "SCeVoZwrS8Tu"
      }
    }
  ]
}